from cachetools import TTLCache, cached
from datetime import datetime
from .person import Person
import logging
import brotli
import json
import re

log = logging.getLogger(__name__)

STATE_CACHING_SECONDS = 30
STATE_CACHE = TTLCache(maxsize=1, ttl=STATE_CACHING_SECONDS)

class LocationSharing:

    def __init__(self, browser, debug=False):

        self.headers = {'authority': 'www.google.com',
                        'accept-language': 'en-US,en;q=0.9',
                        'accept-encoding': 'gzip, deflate, br',
                        'accept': 'application/json, text/plain, */*',
                        'referer': 'https://www.google.com',
                        'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'}
        self.payload = {'authuser': 1,
                        'hl': 'en',
                        'gl': 'us',
                        'authuser': 1,
                        'pb': '!1e1!2m2!1sp-PDW8CZOKj_0gKPorqABQ!7e81!3m2!1s107699590211062977112!2s'}
        self.url = 'https://www.google.com/maps/preview/locationsharing/read'
        self.r = browser
        self.data = None
        self.debug = debug
        self.people = []

    def query_data(self):

        log.debug('Query Data - Requesting location data.')
        response = self.r.get(self.url, params=self.payload, headers=self.headers)
        log.debug('Query Data - Decompressing and decoding response.')
        self.data = brotli.decompress(response.content).decode('utf-8')

    def clean_data(self):

        log.debug('Data Cleanup - Moving data to local variable.')
        dirty_data = self.data
        log.debug('Data Cleanup - Clearing contents of self.data.')
        self.data = []

        log.debug('Data Cleanup - Splitting output by person per row.')
        # seems to be the best split to get person per row
        dirty_data = dirty_data.split(sep='[[')[2:]

        log.debug('Data Cleanup - Deleting useless/unknown content.')
        people = []
        for person in dirty_data:
            person = person.split(sep='"')
            # rows with useful information
            keep_rows = [1, 3, 5, 8, 9, 11, 22]
            delete_rows = []
            for row in range(len(person)):
                if row not in keep_rows:
                    delete_rows.append(row)
            for trash in sorted(delete_rows, reverse=True):
                del person[trash]
            people.append(person)

        log.debug('Data Cleanup - Creating dict for each person in output.')
        counter = 0
        for person in people:
            person_data = {}
            person_data['id'] = int(person[0])
            person_data['photo'] = person[1]
            person_data['full_name'] = person[2]
            person_data['first_name'] = person[2].split()[0]
            person_data['address'] = person[4]
            person_data['country'] = person[5]
            person_data['last_seen'] = int(person[3].split(',')[5]) / 1000

            gps = re.search(r'(^.*\[null,)([-,0-9,\.].*,[0-9,\.].*)(\]\n)',
                                        person[3], flags=16).group(2).split(',')
            person_data['gps'] = {  'longitude':gps[0],
                                    'latitude':gps[1],
                                    'accuracy': int(person[3].split(',')[6])}

            # TODO: the module crashes here from time to time. error:
            # AttributeError: 'NoneType' object has no attribute 'group'
            # wonder if it has something to do with battery not being returned
            # sometimes. first step is to output raw response from google to
            # file for analysis. Create try based on contents which causes error.
            battery = re.search(r'(^.*\[)([0-9]{1},[0-9]*)',
                                        person[6], flags=16).group(2).split(',')

            if int(battery[0]) == 0:
                person_data['battery'] = {  'charging': False,
                                            'percent': int(battery[1])}
            else:
                person_data['battery'] = {  'charging': True,
                                            'percent': int(battery[1])}

            self.data.append(person_data)
            counter += 1
        log.debug('Data Cleanup - Successfully stored output for {} people.'.format(counter))

    def create_people(self):

        log.debug('Create People - Clearing previously stored people')
        self.people = []

        log.debug('Create People - Converting location data into Person object.')
        for person in self.data:
            self.people.append(Person(person))

    # TODO: should probably modify cache duration based on refresh interval
    # from configuration.yaml. consideration would be if ttl is shorter than
    # refresh interval possibly? need to dig into this functionality a bit more
    # to determine best course of action.
    @cached(STATE_CACHE)
    def update(self):

        log.info('Performing data query.')
        self.query_data()
        log.info('Performing data cleanup.')
        self.clean_data()
        log.info('Creating people.')
        self.create_people()

        if self.debug:
            if self.data and len(self.data) != 0:
                log.info('Dumping location data for {} people.'.format(len(self.data)))
                for person in self.data:
                    print(json.dumps(person, sort_keys=False, indent=2))
