from cachetools import TTLCache, cached
from .localization import Localize
from datetime import datetime
from .person import Person
from pathlib import Path
import logging
import brotli
import json
import re

log = logging.getLogger(__name__)

STATE_CACHING_SECONDS = 30
STATE_CACHE = TTLCache(maxsize=1, ttl=STATE_CACHING_SECONDS)

class LocationSharing:

    def __init__(self, browser, config_path, localization='US', debug=False):

        self.debug = debug
        self.output_clean = []
        self.output_dirty = None
        self.output_raw = None
        self.people = []
        self.r = browser
        self.url = Localize(location=localization)

        self.debug_folder = Path(config_path) / Path('debug')
        self.debug_file = self.debug_folder / Path('raw_output_debug')

        self.headers = {'authority': self.url.location_header_authority,
                        'accept-language': 'en-US,en;q=0.9',
                        'accept-encoding': 'gzip, deflate, br',
                        'accept': 'application/json, text/plain, */*',
                        'referer': self.url.location_header_referer,
                        'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'}
        self.payload = {'authuser': 1,
                        'hl': 'en',
                        'gl': 'us',
                        'authuser': 1,
                        'pb': '!1e1!2m2!1sp-PDW8CZOKj_0gKPorqABQ!7e81!3m2!1s107699590211062977112!2s'}

    def query_data(self):

        log.debug('Query Data - Requesting location data.')
        response = self.r.get(self.url.location_sharing_url, params=self.payload, headers=self.headers)
        log.debug('Query Data - Decompressing and decoding response.')
        self.output_raw = brotli.decompress(response.content).decode('utf-8')

    def clean_data(self):

        log.debug('Data Cleanup - Moving data to local variable.')
        self.output_dirty = self.output_raw
        log.debug('Data Cleanup - Clearing contents of self.data.')

        log.debug('Data Cleanup - Splitting output by person per row.')
        # seems to be the best split to get person per row
        self.output_dirty = self.output_dirty.split(sep='[[')[2:]

        log.debug('Data Cleanup - Deleting useless/unknown content.')
        people = []
        for person in self.output_dirty:
            person = person.split(sep='"')
            # rows with useful information
            keep_rows = [1, 3, 5, 8, 9, 11, 22]
            delete_rows = []
            for row in range(len(person)):
                if row not in keep_rows:
                    delete_rows.append(row)
            for trash in sorted(delete_rows, reverse=True):
                del person[trash]
            people.append(person)

        log.debug('Data Cleanup - Creating dict for each person in output.')
        counter = 0
        for person in people:
            person_data = {}
            person_data['id'] = int(person[0])
            person_data['photo'] = person[1]
            person_data['full_name'] = person[2]
            person_data['first_name'] = person[2].split()[0]
            person_data['address'] = person[4]
            person_data['country'] = person[5]
            person_data['last_seen'] = int(person[3].split(',')[5]) / 1000

            gps = re.search(r'(^.*\[null,)([-,0-9,\.].*,[0-9,\.].*)(\]\n)',
                                        person[3], flags=16).group(2).split(',')
            person_data['gps'] = {  'longitude':gps[0],
                                    'latitude':gps[1],
                                    'accuracy': int(person[3].split(',')[6])}

            # TODO: the module crashes here from time to time. error:
            # AttributeError: 'NoneType' object has no attribute 'group'
            # wonder if it has something to do with battery not being returned
            # sometimes. first step is to output raw response from google to
            # file for analysis. Create try based on contents which causes error.

            # maybe something like...
            try:
                battery = re.search(r'(^.*\[)([0-9]{1},[0-9]*)',
                                            person[6], flags=16).group(2).split(',')

                if int(battery[0]) == 0:
                    person_data['battery'] = {  'charging': False,
                                                'percent': int(battery[1])}
                else:
                    person_data['battery'] = {  'charging': True,
                                                'percent': int(battery[1])}
            except:
                log.info('Data Cleanup - Issue reading battery info config_pathfor {}.'.format(person_data['first_name']))
                if self.debug_file.exists() is False:
                    if self.debug_folder.exists() is False:
                        self.debug_folder.mkdir(mode=0o770)
                self.debug_file.touch(mode=0o660)
                with open(self.debug_file, mode='a') as debug_output:
                    debug_output.write('{}\n\n'.format(self.output_raw))

            self.output_clean.append(person_data)
            counter += 1
        log.debug('Data Cleanup - Successfully stored output for {} people.'.format(counter))

    def create_people(self):

        log.debug('Create People - Clearing previously stored people')
        self.people = []

        log.debug('Create People - Converting location data into Person object.')
        for person in self.output_clean:
            self.people.append(Person(person))

    # TODO: should probably modify cache duration based on refresh interval
    # from configuration.yaml. consideration would be if ttl is shorter than
    # refresh interval possibly? need to dig into this functionality a bit more
    # to determine best course of action.
    @cached(STATE_CACHE)
    def update(self):

        log.info('Performing data query.')
        self.query_data()
        log.info('Performing data cleanup.')
        self.clean_data()
        log.info('Creating people.')
        self.create_people()

        if self.debug:
            if self.output_clean and len(self.output_clean) != 0:
                log.info('Dumping location data for {} people.'.format(len(self.output_clean)))
                for person in self.output_clean:
                    print(json.dumps(person, sort_keys=False, indent=2))
